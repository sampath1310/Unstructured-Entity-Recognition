{
    "cells": [
        {
            "cell_type": "markdown", 
            "source": "# Unstructured Entity Recognition", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 1, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "from io import BytesIO  \n\ndef put_file(credentials, local_file_name):  \n    \"\"\"This functions returns a StringIO object containing\n    the file content from Bluemix Object Storage V3.\"\"\"\n    f = open(local_file_name,'r')\n    my_data = f.read()\n    url1 = ''.join(['https://identity.open.softlayer.com', '/v3/auth/tokens'])\n    data = {'auth': {'identity': {'methods': ['password'],\n            'password': {'user': {'name': credentials['username'],'domain': {'id': credentials['domain_id']},\n            'password': credentials['password']}}}}}\n    headers1 = {'Content-Type': 'application/json'}\n    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n    resp1_body = resp1.json()\n    for e1 in resp1_body['token']['catalog']:\n        if(e1['type']=='object-store'):\n            for e2 in e1['endpoints']:\n                        if(e2['interface']=='public'and e2['region']=='dallas'):\n                            url2 = ''.join([e2['url'],'/', credentials['container'], '/', local_file_name])\n    s_subject_token = resp1.headers['x-subject-token']\n    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n    resp2 = requests.put(url=url2, headers=headers2, data = my_data )\n    print(resp2)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "import io\nimport requests\nimport zipfile\nimport os\n\nurl=requests.get('https://www.techgig.com/files/DataScienceFullData/11782_100123/TechGig-DataSet.zip')\ndata = zipfile.ZipFile(io.BytesIO(url.content))\ndata.extractall()"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 2, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "['MNB.csv', 'pred2.csv', 'train.csv', 'feat.pickle', 'test_r.csv', 'pred.csv', 'GBN.csv', 'sample-submission.csv', 'important', 'test.csv', 'pred1.csv', 'train_r.csv']\n", 
                    "name": "stdout"
                }
            ], 
            "source": "path = os.getcwd()\nos.chdir(path+'/TechGig DataSet')\nprint os.listdir(os.getcwd())"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 3, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "import pandas as pd\nimport numpy as np\nimport gc"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 4, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "train = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 5, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "(9850, 3)\n(21070, 2)\n", 
                    "name": "stdout"
                }
            ], 
            "source": "print train.shape\nprint test.shape"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 7, 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 7, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>StringToExtract</th>\n      <th>description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>ddlsql144</td>\n      <td>fotrscomi.srsrelvqe.2008.agentjob.jobduration ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>ddlsql43</td>\n      <td>a sql job failed to complete successfully. sql...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13</td>\n      <td>ddlsql43</td>\n      <td>a sql job failed to complete successfully. sql...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>19</td>\n      <td>ddlsql43</td>\n      <td>a sql job failed to complete successfully. sql...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>22</td>\n      <td>fra-sql-03</td>\n      <td>srsrelvqeagent - a sql job failed to complete ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "   id StringToExtract                                        description\n0   3       ddlsql144  fotrscomi.srsrelvqe.2008.agentjob.jobduration ...\n1   7        ddlsql43  a sql job failed to complete successfully. sql...\n2  13        ddlsql43  a sql job failed to complete successfully. sql...\n3  19        ddlsql43  a sql job failed to complete successfully. sql...\n4  22      fra-sql-03  srsrelvqeagent - a sql job failed to complete ..."
                    }
                }
            ], 
            "source": "train.head()"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 8, 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 8, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>request 1 - http://frapub2.intranet.ecnahcdrof...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>auth critical alert detected detected entry:  ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>a sql job failed to complete successfully. sql...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>oionr advanced alert alert me when an interfac...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>a sql job failed to complete successfully. sql...</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "   id                                        description\n0   1  request 1 - http://frapub2.intranet.ecnahcdrof...\n1   2  auth critical alert detected detected entry:  ...\n2   4  a sql job failed to complete successfully. sql...\n3   5  oionr advanced alert alert me when an interfac...\n4   6  a sql job failed to complete successfully. sql..."
                    }
                }
            ], 
            "source": "test.head()"
        }, 
        {
            "cell_type": "markdown", 
            "source": "# Understanding Data", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "len(np.unique(train.StringToExtract))"
        }, 
        {
            "cell_type": "markdown", 
            "source": "There are 1311 unique lables in training set.Out of Which top five labled in dataset  most are\n\nddlsql43                                    3817\n\nesc-ev-11                                    442\n\nnycpub                                       115\n\nddlsql37                                     109\n\nddl-mocsrds-01                               101", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "# Processing Text\n0. Combine train and text files to procress data at once\n\n1. Make description to lowercase \n\n2. Remove urls from the description\n\n3. Removing date and time from description\n", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 9, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "import nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag\nimport re\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import word_tokenize ,sent_tokenize"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 10, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "#combine train and test dataset for preprocessing data\ntest = test.assign(StringToExtract='NA')\ndf=train.append(test,ignore_index=True)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 11, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "#Remove url and date and time,lower case\nstop = nltk.corpus.stopwords\nlistofstops=set(stop.words('english'))\nwordnet_lemmatizer = WordNetLemmatizer()\npostem=PorterStemmer()"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "def lower_set(x):\n    return x.decode('utf-8').lower()\ndef remove_url(x):\n    return re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', x)\ndef remove_date_time(x):\n    td=re.compile(r'\\d{1,2}[-/]\\d{1,2}[-/]\\d{4}|\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}|\\d{1,2}[:]\\d{1,2}[:]\\d{1,2}[\\s][paPA][mM]|\\d{1,2}[:]\\d{1,2}[:]\\d{1,2}|\\d{1,2}[:]\\d{1,2}')#10-3-2017\n    return re.sub(td,'',x)\ndef del_stopwords(x):\n    return ' '.join([i for i in word_tokenize(x) if i not in listofstops])\ndef stem_doc(x):\n    red_text = [postem.stem(word.strip()) for word in x.split(\" \") if word.strip()!='']\n    return ' '.join(red_text)\ndef clean_words(words):\n    if words:\n        words=words.replace('.',' ')\n        words = words.replace('\\t',' ')\n        words = words.replace('\\r',' ')\n        words = words.replace('\\n',' ')\n        words = words.replace(',',' ')\n        words = words.replace(':',' ')\n        words = words.replace(';',' ')\n        words = words.replace('=',' ')\n        #words = words.replace('\\x92','') # apostrophe encoding\n        #words = words.replace('\\x08','\\\\b') # \\b is being treated as backspace\n        #words = ''.join([i for i in words if not i.isdigit()])\n        words = words.replace('_',' ')\n        words = words.replace('(',' ')\n        words = words.replace(')',' ')\n        words = words.replace('+',' ')\n        #words = words.replace('-',' ')\n        words = words.replace('`',' ')\n        words = words.replace('\\'',' ')\n        #words = words.replace('.',' ')\n        words = words.replace('#',' ')\n        words = words.replace('/',' ')\n        words = words.replace('_',' ')\n        words = words.replace('\"',' ')\n        words = words.replace('$',' ')\n        words = words.replace('}',' ')\n        words = words.replace('{',' ')\n    return words.strip()\ndef final_clean(x):\n    return ' '.join([i.strip() for i in x.split()])\n##REmoving CD i.e  \tCardinal number \ndef remove_cd(sent):\n    tag=pos_tag(sent.split())\n    return ' '.join([i for i,j in tag if j!='CD'])\n##Removing words like na 225 internal as they occur in almost all data record\nlists=['na','internal','225']\ndef remove_internal_na(sent):\n    return ' '.join([i for i in sent.split(' ') if i not in lists])"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true, 
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "df = df.assign(description=df.description.apply(lambda x: lower_set(x)))\ndf = df.assign(description=df.description.apply(lambda x: remove_url(x)))\ndf=df.assign(description=df.description.apply(lambda x: remove_date_time(x)))\ndf=df.assign(description=df.description.apply(lambda x: del_stopwords(x)))\ndf=df.assign(description=df.description.apply(lambda x: stem_doc(x)))\ndf=df.assign(description=df.description.apply(lambda x:clean_words(x)))\ndf.description = df.description.apply(lambda x:x.encode('utf-8'))\n#df=df.assign(description=df.description.apply(lambda x:remove_cd(x)))\ndf=df.assign(description=df.description.apply(lambda x:final_clean(x)))\ndf=df.assign(description=df.description.apply(lambda x:remove_internal_na(x)))"
        }, 
        {
            "cell_type": "markdown", 
            "source": "Try to use different feature extraction like TfidfVectorizer,CountVectorizer,HashingVectorizer\nout of which CountVectorizer has best result", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "#from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n#from sklearn.feature_extraction.text import HashingVectorizer"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "#Getting words from the train description to make wordvector \nfive_g=CountVectorizer(vocabulary=np.unique(train.StringToExtract))\nfive_g.fit(df.description)\nsomenew=five_g.fit_transform(df.description)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "#seperating train and test dataset for prediction\ndf=pd.DataFrame(somenew.toarray())\ndf.columns=five_g.get_feature_names()\nnewtrain=df.loc[:len(train)-1]\nnewtest=df.loc[len(train):]"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "#The given dataset is highly imbalanced balancing the subsample is one approach\ndef balanced_subsample(y, size=None)\n    subsample = []\n    if size is None:\n        n_smp = y.value_counts().min()\n    else:\n        n_smp = int(size / len(y.value_counts().index))\n\n    for label in y.value_counts().index:\n        samples = y[y == label].index.values\n        index_range = range(samples.shape[0])\n        indexes = np.random.choice(index_range, size=n_smp, replace=False)\n        subsample += samples[indexes].tolist()\n    return subsample"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "newtrain=train.loc[balanced_subsample(train.StringToExtract)]\nbalanced_train=pd.concat([train]*200)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\nmodel=gnb.fit(newtrain,train.StringToExtract)\npred=model.predict(newtest)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "sol=pd.DataFrame(pred)\nsol.to_csv('pred.csv')\nfinalsol =pd.read_csv('sample-submission.csv')\n\nfinalsol=finalsol.assign(StringToExtract=pred)\n\nfinalsol.to_csv('GBN.csv',quoting=False,index=False)"
        }, 
        {
            "cell_type": "markdown", 
            "source": "The naive bayes has the best accuracy when compared to other model(rf,xgboost,svm) with accuracy around 90.8", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "###### Below are some of the techniques which can improve the model but in this case these were not helpful", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "processed_train=df[df.StringToExtract !='NA']\nprocessed_test=df[df.StringToExtract =='NA']"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "valuescount=processed_train.StringToExtract.value_counts()\n\nreplicate_subset_list=processed_train.StringToExtract.value_counts()[(processed_train.StringToExtract.value_counts()==1)].index.tolist()\n\nreplicate_subset=processed_train[processed_train.StringToExtract.apply(lambda x: x in replicate_subset_list)]\n\nreplicated_set=processed_train.loc[np.repeat(replicate_subset.index.values,repeats=5)]"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "from nltk.probability import FreqDist\n\nfeq=FreqDist()\n\ndoc=[]\n\ndf.description.apply(lambda x: doc.append(x))\n\nfdist = FreqDist(word.lower() for word in word_tokenize(' '.join(doc)))\n\nlists=[i for i,j in fdist.most_common()]##Removing most common words like net ,internal and so on \n##one label is missing in those words key role missing ddl43"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "s=[ i for i in lists if i not in lables]"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "def search(sent):\n    sent=' '.join(sent.split('.'))\n    lis=[i for i in sent.split() if i  in lables]\n    return ' '.join(lis)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "df.description=df.description.apply(lambda x: search(x))"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "## MAKE NEW CLASSES BASED ON NET,COM BECASE THERE ARESOME CLASSEES THAT ARE MISLABLED\ntrain[train.StringToExtract=='knowwho'].description.loc[2972]"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "lables=np.unique(train.StringToExtract)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "newprocessedtrain=processed_train.append(replicated_set,ignore_index=True)\n\nnewprocessedtrain.description=newprocessedtrain.description.apply(lambda x: ' '.join([ i for i in x.split() if i not in lists]))"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "totaldf=newprocessedtrain.description.append(processed_test.description)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "#tfv=TfidfVectorizer(max_features=500)\ntfv=CountVectorizer(max_features=3000)\ntfv.fit(newprocessedtrain.description,newprocessedtrain.StringToExtract)\n\nfeat_ext=tfv.fit_transform(totaldf)\n\nnewfeat=feat_ext.toarray()\n\nnewtrain=newfeat[:12320]\nnewtest=newfeat[12320:]"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "newdf =pd.DataFrame(newfeat)\nnewdf=newdf.assign(StringToExtract=df.StringToExtract)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "newtrain=newdf.loc[newdf.StringToExtract != 'NA']\nnewtest=newdf.loc[newdf.StringToExtract == 'NA']\nnewtest.drop(['StringToExtract'],axis=1,inplace=True)\n\nnewtrain.drop(['StringToExtract'],axis=1,inplace=True)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "import re\n\nnet=re.compile(pattern=r\"\"\"[-\\w\\.]*\\.net\"\"\")\ncom=re.compile(pattern=r\"\"\"([[\\w\\.-]*]*\\.com)\"\"\")\ndash=re.compile(pattern=r\"\"\"[\\w-]*\"\"\")\n\ndef find_net_com(sent):\n    lis=[]\n    lis=net.findall(sent)    \n    lis.append(' '.join(com.findall(sent)))\n    lis.append(' '.join(dash.findall(sent)))\n    return ' '.join(lis)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "newtrain=train.description.apply(lambda x:find_net_com(x))\nnewtest=test.description.apply(lambda x:find_net_com(x))"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "df=newtrain.append(newtest)\n\nfinal =pd.DataFrame(feat.toarray())\n\nfinaltrain =final.loc[:len(train)-1]\nfinaltest=final.loc[len(train):]"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "def chunking(sent):\n    \n    chunkGram = r\"\"\"Chunk: {<NN|VBP|JJ><JJ><JJ><JJ>}\n                    Chunk:{<JJ>\\.<NN>\\.<NN>\\.<NN>}\n                    Chunk:  {<JJ><NN><JJ><JJ>}  \n                    Chunk: {<CD>\\.<CD>\\.<CD>\\.<CD>}\n                    Chunk : {<JJ>\\.<IN><NN>\\.<NN>\\.<NN>}\"\"\"\n    chunkParser = nltk.RegexpParser(chunkGram)\n    chunked = chunkParser.parse(pos_tag(sent.split()))\n    res =''\n    for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n                    res+= str(subtree)\n    return res"
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "language": "python", 
            "name": "python2-spark20", 
            "display_name": "Python 2 with Spark 2.0"
        }, 
        "language_info": {
            "version": "2.7.11", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat_minor": 1
}